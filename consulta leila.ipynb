{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTRODUCCIONES GENERALES\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "#%%\n",
    "def ldata(archive):\n",
    "    f=open(archive)\n",
    "    data=[]\n",
    "    for line in f:\n",
    "        line=line.strip()\n",
    "        col=line.split()\n",
    "        data.append(col)\n",
    "    return data\n",
    "#%%\n",
    "prot = ldata('C:/Users/Admin/Documents/GitHub/Redes_TP2/data/yeast_AP-MS.txt')\n",
    "bina = ldata('C:/Users/Admin/Documents/GitHub/Redes_TP2/data/yeast_Y2H.txt')\n",
    "lit = ldata('C:/Users/Admin/Documents/GitHub/Redes_TP2/data/yeast_LIT.txt')\n",
    "litreg = ldata('C:/Users/Admin/Documents/GitHub/Redes_TP2/data/yeast_LIT_Reguly.txt')\n",
    "essential = ldata('C:/Users/Admin/Documents/GitHub/Redes_TP2/data/Essential_ORFs_paperHe.txt')\n",
    "#%% LIMPIO LOS DATOS\n",
    "ess = [a[1] for a in essential[2:1158]]\n",
    "litr = [a[0:2] for a in litreg[1:]]\n",
    "#%%\n",
    "P = nx.Graph()\n",
    "P.add_edges_from(prot)\n",
    "\n",
    "B = nx.Graph()\n",
    "B.add_edges_from(bina)\n",
    "\n",
    "L = nx.Graph()\n",
    "L.add_edges_from(lit)\n",
    "\n",
    "LR = nx.Graph()\n",
    "LR.add_edges_from(litr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#les doy esencialidad como atributo\n",
    "def daresencialidad(redes, ess):\n",
    "    R = redes\n",
    "    esencial = []#np.empty_like((len(R), 1)) ##vamos a tener que usar un diccionario porque hay distinto numero de nodos\n",
    "    for i in range(len(R)):\n",
    "        D = dict()\n",
    "        a = list(R[i].nodes())\n",
    "        for j in range(len(a)):\n",
    "            D[j] = 0\n",
    "            for l in range(len(ess)):\n",
    "                if a[j] == ess[l]:\n",
    "                    D[j] = 1\n",
    "                    break\n",
    "        esencial.append(D)\n",
    "    \n",
    "    return esencial\n",
    "#%% #meto el atributo esencialidad en los nodos de cada red\n",
    "def atribuir(redes, ess):\n",
    "    R = redes\n",
    "    for i in range(len(R)):\n",
    "        for n,e,g in zip(R[i].nodes, daresencialidad(redes, ess)[i].values(), [a[1] for a in list(R[i].degree())]):\n",
    "            R[i].nodes[n]['Esencialidad'] = e\n",
    "            R[i].nodes[n]['Grado'] = g\n",
    "            \n",
    "#%%\n",
    "atribuir([P,B,L,LR],ess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me quedo con el nombre de los nodos esenciales y no esenciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = [P,B,L,LR]\n",
    "R_GC = np.empty_like(R)\n",
    "for i in range(len(R)): #me quedo con la componente gigante de cada red\n",
    "        R_GC[i] = max(nx.connected_component_subgraphs(R[i]), key=len)\n",
    "\n",
    "nodos_esenciales = np.empty(4,dtype = object)\n",
    "grados_esenciales = np.empty_like(nodos_esenciales)\n",
    "nodos_no_esenciales = np.empty(4, dtype = object)\n",
    "grados_no_esenciales = np.empty_like(nodos_no_esenciales)\n",
    "\n",
    "for i in range(len(R_GC)):\n",
    "    nodos_esenciales[i] = [nodos[0] for nodos in list(sorted(R_GC[i].nodes.data(),key = lambda x: -x[1]['Esencialidad']))][0:np.sum([c[1]['Esencialidad'] for c in list(R_GC[i].nodes.data())])]\n",
    "    grados_esenciales[i] = [nodos[1]['Grado'] for nodos in list(sorted(R_GC[i].nodes.data(),key = lambda x: -x[1]['Esencialidad']))][0:np.sum([c[1]['Esencialidad'] for c in list(R_GC[i].nodes.data())])]\n",
    "    nodos_no_esenciales[i] = [nodos[0] for nodos in list(sorted(R_GC[i].nodes.data(),key = lambda x: -x[1]['Esencialidad']))][np.sum([c[1]['Esencialidad'] for c in list(R_GC[i].nodes.data())]):]\n",
    "    grados_no_esenciales[i] = [nodos[1]['Grado'] for nodos in list(sorted(R_GC[i].nodes.data(),key = lambda x: -x[1]['Esencialidad']))][np.sum([c[1]['Esencialidad'] for c in list(R_GC[i].nodes.data())]):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armo tuplas de nodo(nombre) y su grado, para que corra bien la manera de ordenarlos en orden decreciente de grado, en ambos casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NG_es = [[],[],[],[]]\n",
    "NG_nes = [[],[],[],[]]\n",
    "\n",
    "def takeSecond(elem):\n",
    "    return elem[1]\n",
    "\n",
    "for i in range(len(NG_es)):\n",
    "    for nodo, grado in zip(nodos_esenciales[i],grados_esenciales[i]):\n",
    "        NG_es[i].append([nodo,grado])\n",
    "    NG_es[i].sort(reverse=True, key= takeSecond)\n",
    "    \n",
    "for i in range(len(NG_nes)):\n",
    "    for nnodo, ngrado in zip(nodos_no_esenciales[i],grados_no_esenciales[i]):\n",
    "         NG_nes[i].append([nnodo,ngrado])\n",
    "    NG_nes[i].sort(reverse = True,key=takeSecond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defino una función que me compare los grados de los nodos 1 a 1. Si ve que el grado esencial es mayor al no esencial en ese lugar, saca lo mejor que puede (es decir, ese valor de grado no esencial ya que están de mayor a menor). Si el grado esencial es menor al no esencial, entonces le pido que saque, con igual chance, algun nodo que tenga grado entre el no esencial (mayor en este caso) y el esencial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "eliminados= [max(nx.connected_component_subgraphs(P), key=len), max(nx.connected_component_subgraphs(B), key=len), max(nx.connected_component_subgraphs(L), key=len), max(nx.connected_component_subgraphs(LR), key=len)]\n",
    "#en eliminados tomo las Giant components\n",
    "def randomDelete(eliminados, NG_es, NG_nes):\n",
    "    #veo el tamaño con el que empiezo en cada GC de la red\n",
    "    size_pre_randomdelete = [eliminados[0].number_of_nodes(), eliminados[1].number_of_nodes(), eliminados[2].number_of_nodes(), eliminados[3].number_of_nodes()]\n",
    "    \n",
    "    for i in range(len(R)):\n",
    "        #se que repito variables, perdon!\n",
    "        grado_es= [elem[1] for elem in NG_es[i]] #lista de grado de los nodos esenciales para cada red i\n",
    "        grado_nes = [elem[1] for elem in NG_nes[i]] #lo mismo para los no esenciales de esa misma red\n",
    "        nodo_es= [elem[0] for elem in NG_es[i]] #nombre de los nodos es\n",
    "        nodo_nes = [elem[0] for elem in NG_nes[i]] #nombre de los no esenciales\n",
    "    \n",
    "        for r in range(len(nodo_es)):\n",
    "            if grado_es[r] >= grado_nes[r]:\n",
    "                opciones1 = [b[0] for b in NG_nes[i] if b[1]==grado_nes[r]]  #sacame lo mejor que tengas disponible\n",
    "                if len(opciones1)==1:\n",
    "                    eliminados[i].remove_nodes_from(opciones1)\n",
    "                else:\n",
    "                    shuffle(opciones1)\n",
    "                    eliminados[i].remove_nodes_from(opciones1[0])\n",
    "            else: #es decir, si el grado esencial es mas chico\n",
    "                opciones = [b[0] for b in NG_nes[i] if (b[1]<=grado_nes[r] and b[1]>=grado_es[r])] #dale igual oportunidad a los nodos de grado de la no esencial, hasta tan poco grado como el esencial en ese lugar\n",
    "                if len(opciones)==1:\n",
    "                    eliminados[i].remove_nodes_from(opciones)\n",
    "                else:\n",
    "                    shuffle(opciones)\n",
    "                    eliminados[i].remove_nodes_from(opciones[0])\n",
    "                \n",
    "    size_post_randomdelete = [max(nx.connected_component_subgraphs(eliminados[0]), key=len).number_of_nodes(), max(nx.connected_component_subgraphs(eliminados[1]), key=len).number_of_nodes(), max(nx.connected_component_subgraphs(eliminados[2]), key=len).number_of_nodes(), max(nx.connected_component_subgraphs(eliminados[3]), key=len).number_of_nodes()]\n",
    "    return size_pre_randomdelete, size_post_randomdelete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo hago muchas veces para obtener una distribucion de tamaños posibles al sacar nodos al azar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "sizes = []\n",
    "n = 0\n",
    "for n in range(10):\n",
    "    eliminados= [max(nx.connected_component_subgraphs(P), key=len), max(nx.connected_component_subgraphs(B), key=len), max(nx.connected_component_subgraphs(L), key=len), max(nx.connected_component_subgraphs(LR), key=len)]\n",
    "    sizes.append(randomDelete(eliminados, NG_es, NG_nes))     \n",
    "    n= n+1\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([1004, 1647, 1213, 3224], [996, 1605, 1210, 3040]),\n",
       " ([1004, 1647, 1213, 3224], [996, 1605, 1210, 3040]),\n",
       " ([1004, 1647, 1213, 3224], [996, 1605, 1210, 3040]),\n",
       " ([1004, 1647, 1213, 3224], [996, 1605, 1210, 3040]),\n",
       " ([1004, 1647, 1213, 3224], [996, 1605, 1210, 3040]),\n",
       " ([1004, 1647, 1213, 3224], [996, 1605, 1210, 3040]),\n",
       " ([1004, 1647, 1213, 3224], [996, 1605, 1210, 3040]),\n",
       " ([1004, 1647, 1213, 3224], [996, 1605, 1210, 3040]),\n",
       " ([1004, 1647, 1213, 3224], [996, 1605, 1210, 3040]),\n",
       " ([1004, 1647, 1213, 3224], [996, 1605, 1210, 3040])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes  #aca tenemos tamaño redes{p,b,l,lr} [[antes de sacarles nodos], [despues]] para cada iteracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodo_es= [elem[0] for elem in NG_es[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodo_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca no tiene sentido, porque tengo 406 nodos esenciales en la primera red, por lo que tendría que tener, cuando mínimo, 1004( nodos iniciales) - 406 (esenciales) en la red, asumiendo que los que elimino no desarman la GC.\n",
    "Ahora bien, empiezo con 1004 y termino con 996 nodos, osea que eliminó 8 nodos a lo sumo (eso solo en la primera red- es decir, primer lugar de cada lista]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
